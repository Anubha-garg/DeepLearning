{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the softmax activation function\n",
    "def softmax(z):\n",
    "    z_exp = np.exp(z)\n",
    "    z_sum = np.sum(z_exp, axis=0,keepdims=True)\n",
    "    return z_exp / z_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the sigmoidGradient function\n",
    "def sigmoidGradient(A):\n",
    "    return A * (1-A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the tanh activation function\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the tanh gradient function\n",
    "def tanhGradient(A):\n",
    "    return 1 - np.square(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the ReLu activation function\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Relu gradient function\n",
    "def reluGradient(A):\n",
    "    A[A < 0] = 0\n",
    "    A[A >= 0] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Leaky Relu function\n",
    "def leakyrelu(z):\n",
    "    return np.maximum(0.01*z , z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LeakyRelu Gradient function\n",
    "def leakyreluGradient(A):\n",
    "    A[A < 0] = 0.01\n",
    "    A[A >= 0] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X,activations,parameters,keep_prob):\n",
    "    \n",
    "    cache={}\n",
    "    cache[\"A0\"]=X\n",
    "    n=len(parameters)//2\n",
    "    \n",
    "    if keep_prob[0] < 1:\n",
    "            Ashape = cache[\"A0\"].shape\n",
    "            cache[\"D0\"] = np.random.rand(Ashape[0],Ashape[1])\n",
    "            cache[\"D0\"] = (cache[\"D0\"] < keep_prob[0]).astype(int).astype(int)\n",
    "            cache[\"A0\"] = cache[\"A0\"] * cache[\"D0\"]\n",
    "            cache[\"A0\"] = cache[\"A0\"] / keep_prob[0]          \n",
    "    \n",
    "    for i in range(1,n+1):\n",
    "        cache[\"Z\"+str(i)] = np.dot(parameters[\"W\"+str(i)],cache[\"A\"+str(i-1)]) + parameters[\"b\"+str(i)]\n",
    "        \n",
    "        if activations[i-1]==\"relu\":\n",
    "            cache[\"A\"+str(i)] = relu(cache[\"Z\"+str(i)])\n",
    "        elif activations[i-1]==\"tanh\":\n",
    "            cache[\"A\"+str(i)] = tanh(cache[\"Z\"+str(i)])\n",
    "        elif activations[i-1]==\"sigmoid\":\n",
    "            cache[\"A\"+str(i)] = sigmoid(cache[\"Z\"+str(i)])\n",
    "        elif activations[i-1]==\"leakyrelu\":\n",
    "            cache[\"A\"+str(i)] = leakyrelu(cache[\"Z\"+str(i)])\n",
    "        elif activations[i-1]==\"softmax\":\n",
    "            cache[\"A\"+str(i)] = softmax(cache[\"Z\"+str(i)])\n",
    "            \n",
    "        # For dropout regularization\n",
    "        if keep_prob[i] < 1:\n",
    "            Ashape = cache[\"A\"+str(i)].shape\n",
    "            cache[\"D\"+str(i)] = np.random.rand(Ashape[0],Ashape[1])\n",
    "            cache[\"D\"+str(i)] = (cache[\"D\"+str(i)] < keep_prob[i]).astype(int)\n",
    "            cache[\"A\"+str(i)] = cache[\"A\"+str(i)] * cache[\"D\"+str(i)]\n",
    "            cache[\"A\"+str(i)] = cache[\"A\"+str(i)] / keep_prob[i]                 \n",
    "    \n",
    "            \n",
    "    Yhat = cache[\"A\"+str(i)]\n",
    "    return cache ,Yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cost without Regularization\n",
    "def computeCost(Yhat,Y,lambd,parameters,softmax):\n",
    "    m=Y.shape[1]\n",
    "    logLoss = -(Y * np.log(Yhat))\n",
    "    if not softmax:\n",
    "        logLoss += -((1-Y) * np.log(1-Yhat))\n",
    "    cost = (np.sum(logLoss))/m\n",
    "    \n",
    "    if lambd !=0:\n",
    "        n = len(parameters)//2\n",
    "        L2_regularization_cost=0\n",
    "        for i in range(1,n+1):\n",
    "            L2_regularization_cost +=  np.sum(np.square(parameters[\"W\"+str(i)]))\n",
    "            \n",
    "        L2_regularization_cost *= lambd / (2 * m)\n",
    "        cost += L2_regularization_cost\n",
    "    \n",
    "    return cost    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropagation(parameters,cache,X,Y,lambd,keep_prob):\n",
    "    m=Y.shape[1]\n",
    "    n = len(parameters)//2 \n",
    "    Yhat = cache[\"A\"+str(n)]\n",
    "    if activations[n-1]==\"softmax\":\n",
    "        dYhat = -Y/Yhat\n",
    "    else: #sigmoid\n",
    "        dYhat = -Y/Yhat + (1-Y)/(1-Yhat)\n",
    "    grads={}\n",
    "    \n",
    "    grads[\"dA\" + str(n)] = dYhat\n",
    "    if keep_prob[n] < 1:\n",
    "        grads[\"dA\" + str(n)] = (grads[\"dA\" + str(n)] * cache[\"D\" + str(n)])/ keep_prob[n]\n",
    "    \n",
    "    for i in range(n,0,-1): \n",
    "        \n",
    "        activationGradient = np.zeros(cache[\"A\"+str(i)].shape)\n",
    "        \n",
    "        if activations[i-1]==\"sigmoid\":\n",
    "            activationGradient = sigmoidGradient(cache[\"A\"+str(i)])\n",
    "        elif activations[i-1]==\"relu\":\n",
    "            activationGradient = reluGradient(cache[\"A\"+str(i)])\n",
    "        elif activations[i-1]==\"leakyrelu\":\n",
    "            activationGradient = leakyreluGradient(cache[\"A\"+str(i)])\n",
    "        elif activations[i-1]==\"tanh\":\n",
    "            activationGradient = tanhGradient(cache[\"A\"+str(i)])       \n",
    "            \n",
    "        \n",
    "        if activations[i-1]==\"softmax\":\n",
    "            grads[\"dZ\" + str(i)] = cache[\"A\"+str(i)] - Y            \n",
    "        else:\n",
    "            grads[\"dZ\" + str(i)] = activationGradient * grads[\"dA\" + str(i)]\n",
    "            \n",
    "        grads[\"dW\" + str(i)] = np.dot(grads[\"dZ\" + str(i)] , np.transpose(cache[\"A\"+str(i-1)]))/m\n",
    "        grads[\"db\" + str(i)] = np.sum(grads[\"dZ\" + str(i)] , axis=1,keepdims=True)/m\n",
    "        grads[\"dA\" + str(i-1)] = np.dot(np.transpose(parameters[\"W\"+str(i)]) ,grads[\"dZ\" + str(i)]) \n",
    "        \n",
    "        if keep_prob[i-1] < 1:\n",
    "            grads[\"dA\" + str(i-1)] = (grads[\"dA\" + str(i-1)] * cache[\"D\" + str(i-1)])/keep_prob[i-1]\n",
    "        \n",
    "        if lambd != 0:\n",
    "            grads[\"dW\" + str(i)] = grads[\"dW\" + str(i)] + (lambd/m * parameters[\"W\" + str(i)])\n",
    "            \n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParametersWithAdam(parameters, grads, learning_rate,beta1,beta2,t,epsilon,V,S):\n",
    "    \n",
    "    n = len(parameters)//2 #number of layers\n",
    "    for i in range(1,n+1):\n",
    "               \n",
    "        V[\"Vdw\"+str(i)] = beta1 * V[\"Vdw\"+str(i)] + (1-beta1) * grads[\"dW\" + str(i)]\n",
    "        V[\"Vdb\"+str(i)] = beta1 * V[\"Vdb\"+str(i)] + (1-beta1) * grads[\"db\" + str(i)]\n",
    "        \n",
    "        S[\"Sdw\"+str(i)] = beta2 * S[\"Sdw\"+str(i)] + (1-beta2) * grads[\"dW\" + str(i)] * grads[\"dW\" + str(i)]\n",
    "        S[\"Sdb\"+str(i)] = beta2 * S[\"Sdb\"+str(i)] + (1-beta2) * grads[\"db\" + str(i)] * grads[\"db\" + str(i)]\n",
    "        \n",
    "        VdwCorrected = V[\"Vdw\"+str(i)]/(1- np.power(beta1,t))\n",
    "        SdwCorrected = S[\"Sdw\"+str(i)]/(1- np.power(beta2,t))\n",
    "        VdbCorrected = V[\"Vdb\"+str(i)]/(1- np.power(beta1,t))\n",
    "        SdbCorrected = S[\"Sdb\"+str(i)]/(1- np.power(beta2,t))        \n",
    "        \n",
    "        parameters[\"W\"+str(i)] -= learning_rate * VdwCorrected / np.sqrt(SdwCorrected +epsilon)\n",
    "        parameters[\"b\"+str(i)] -= learning_rate * VdbCorrected / np.sqrt(SdbCorrected +epsilon)\n",
    "   \n",
    "    return parameters,V,S "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeAdam(parameters):    \n",
    "    n = len(parameters)//2 #number of layers\n",
    "    V={}\n",
    "    S={}    \n",
    "    for i in range(1,n+1):               \n",
    "        V[\"Vdw\"+str(i)] = np.zeros(parameters[\"W\" + str(i)].shape)\n",
    "        V[\"Vdb\"+str(i)] = np.zeros(parameters[\"b\" + str(i)].shape) \n",
    "        S[\"Sdw\"+str(i)] = np.zeros(parameters[\"W\" + str(i)].shape) \n",
    "        S[\"Sdb\"+str(i)] = np.zeros(parameters[\"b\" + str(i)].shape) \n",
    "    return V,S   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intitializeMomentum(parameters):    \n",
    "    V={}    \n",
    "    n = len(parameters)//2 #number of layers\n",
    "    for i in range(1,n+1):               \n",
    "        V[\"Vdw\"+str(i)] = np.zeros(parameters[\"W\" + str(i)].shape)\n",
    "        V[\"Vdb\"+str(i)] = np.zeros(parameters[\"b\" + str(i)].shape)        \n",
    "    return V    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParametersWithMomentum(parameters, grads, learning_rate,beta1,V):\n",
    "    n = len(parameters)//2 #number of layers\n",
    "    for i in range(1,n+1):\n",
    "               \n",
    "        V[\"Vdw\"+str(i)] = beta1 * V[\"Vdw\"+str(i)] + (1-beta1) * grads[\"dW\" + str(i)]\n",
    "        V[\"Vdb\"+str(i)] = beta1 * V[\"Vdb\"+str(i)] + (1-beta1) * grads[\"db\" + str(i)]\n",
    "        \n",
    "        parameters[\"W\"+str(i)] -=  learning_rate * V[\"Vdw\"+str(i)]\n",
    "        parameters[\"b\"+str(i)] -=  learning_rate * V[\"Vdb\"+str(i)]\n",
    "   \n",
    "    return parameters,V   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(parameters, grads, learning_rate):\n",
    "    n = len(parameters)//2 #number of layers\n",
    "    for i in range(1,n+1):\n",
    "        parameters[\"W\"+str(i)] = parameters[\"W\"+str(i)] - learning_rate * grads[\"dW\" + str(i)]\n",
    "        parameters[\"b\"+str(i)] = parameters[\"b\"+str(i)] - learning_rate * grads[\"db\" + str(i)]\n",
    "   \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeParameters(layers,activation):\n",
    "    parameters={}\n",
    "    for i in range(1,len(layers)):\n",
    "       \n",
    "        parameters[\"b\"+str(i)] = np.zeros((layers[i],1))\n",
    "        \n",
    "        if activation[i-1]==\"relu\":\n",
    "            parameters[\"W\"+str(i)] = np.random.randn(layers[i],layers[i-1]) * np.sqrt(2/layers[i-1]) # He Initialization     \n",
    "        else:\n",
    "            parameters[\"W\"+str(i)] = np.random.randn(layers[i],layers[i-1]) * np.sqrt(1/layers[i-1]) # Xavier Initialization\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose an option for learning rate decay\n",
    "def learningRateDecay(epoch_num,option):\n",
    "    \n",
    "    alpha0=1.2 #Tunable hyperparameters\n",
    "    k=0.95 #Tunable hyperparameters\n",
    "    \n",
    "    l=0\n",
    "    if option == 1:\n",
    "        l = alpha0 / (1 + k * epoch_num)\n",
    "    elif option == 2:    \n",
    "        l = np.power(k,epoch_num) * alpha0  #Exponential Decay\n",
    "    elif option == 3:    \n",
    "        l = k * alpha0 / np.sqrt(epoch_num)\n",
    "    else:     \n",
    "        # Staircase\n",
    "        t = epoch_num // 100\n",
    "        if t <= 4: l = 1.2\n",
    "        else: l = 0.001\n",
    "    \n",
    "    return l   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomMiniBatches(X,Y,mini_batch_size):\n",
    "    \n",
    "    mini_batches = []\n",
    "    m = X.shape[1]\n",
    "    num_complete_minibatches = m//mini_batch_size\n",
    "    \n",
    "    # Shuffle (X, Y) synchronously    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "    \n",
    "    # Partition (shuffled_X, shuffled_Y). Minus the end case.       \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[:,k * mini_batch_size : (k+1) * mini_batch_size ]\n",
    "        mini_batch_Y = Y[:,k * mini_batch_size : (k+1) * mini_batch_size ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X[:,(k+1) * mini_batch_size:]\n",
    "        mini_batch_Y = Y[:,(k+1) * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Xtrain,Ytrain,x_test,y_test,activations,layers,lambd,keep_prob,\n",
    "          haveLearningRateDecay=False,DecayOption=0,optimizer=\"adam\",mini_batch_size=64,beta1=0.9,beta2=0.999,\n",
    "          epsilon=1e-8,learning_rate=0.001,max_epochs=60000,print_cost=False,print_accuracy=True): \n",
    "    \n",
    "    #Initialize the parameters\n",
    "    parameters = initializeParameters(layers,activations)\n",
    "    if optimizer ==\"momentum\":\n",
    "        V = intitializeMomentum(parameters)\n",
    "    if optimizer == \"adam\":\n",
    "        V,S = initializeAdam(parameters)\n",
    "    \n",
    "    # initializing the counter required for Adam update\n",
    "    t = 0       \n",
    "            \n",
    "    keep_probTest = [1] * len(keep_prob)\n",
    "    improvementpossible = True\n",
    "    i = 0 #Epoch Counter\n",
    "    while(improvementpossible and i < max_epochs):        \n",
    "        \n",
    "        minibatches = randomMiniBatches(Xtrain.to_numpy(), Ytrain, mini_batch_size)\n",
    "        cost_sum =0\n",
    "        \n",
    "        i += 1\n",
    "        for minibatch in minibatches:\n",
    "            \n",
    "            #Get the mini batches\n",
    "            (X, Y) = minibatch \n",
    "            \n",
    "            #Forward Propagation\n",
    "            cache ,Yhat = forwardPropagation(X,activations,parameters,keep_prob)\n",
    "            \n",
    "            #Compute Cost\n",
    "            if print_cost:\n",
    "                cost_sum += computeCost(Yhat,Y,lambd,parameters,activations[-1]==\"softmax\")\n",
    "                    \n",
    "            #Back Propagation\n",
    "            grads = backwardPropagation(parameters,cache,X,Y,lambd,keep_prob)\n",
    "            \n",
    "            #Get the learning rate\n",
    "            if haveLearningRateDecay:\n",
    "                learning_rate = learningRateDecay(i,DecayOption)\n",
    "            \n",
    "            #Update Parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = updateParameters(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters,V = updateParametersWithMomentum(parameters, grads, learning_rate,beta1,V)\n",
    "            elif optimizer ==\"adam\":\n",
    "                t += 1\n",
    "                parameters,V,S = updateParametersWithAdam(parameters, grads, learning_rate,beta1,beta2,t,epsilon,V,S)\n",
    "            \n",
    "        # Print the cost every 1000 epochs\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_sum/len(minibatches))) \n",
    "            \n",
    "        #Print accuracy \n",
    "        if print_accuracy and i % 100 == 0:    \n",
    "            train_acc,test_acc = printTrainTestAccuracy(Xtrain,Ytrain,x_test,y_test,activations,parameters,keep_probTest,i)\n",
    "            #if test_acc >0.99:\n",
    "                #improvementpossible = False\n",
    "                \n",
    "    if not print_accuracy:           \n",
    "        _,_ = printTrainTestAccuracy(Xtrain,Ytrain,x_test,y_test,activations,parameters,keep_probTest,i)        \n",
    "        \n",
    "\n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTrainTestAccuracy(Xtrain,Ytrain,x_test,y_test,activations,parameters,keep_probTest,i):\n",
    "    \n",
    "    train_acc = predictAndEvaluate(Xtrain,Ytrain,activations,parameters,keep_probTest)\n",
    "    test_acc = predictAndEvaluate(x_test,y_test,activations,parameters,keep_probTest)\n",
    "    print(\"Train/test accuracy after epoch %i: %f : %f\" %(i,train_acc,test_acc))\n",
    "    \n",
    "    return train_acc,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictAndEvaluate(X,Y,activations,parameters,keep_probTest):\n",
    "    #Predict \n",
    "    _ ,Yhat =forwardPropagation(X,activations,parameters,keep_probTest)\n",
    "\n",
    "    #evaluation \n",
    "    m = X.shape[1]\n",
    "    yhat = np.argmax(Yhat, axis=0)\n",
    "    y = np.argmax(Y, axis=0)\n",
    "    correct = np.sum(yhat == y) \n",
    "    return correct/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the inputs\n",
    "def normalizeInputs(x_train,epsilon=1e-8):\n",
    "    mean = np.sum(x_train.to_numpy(), axis=1,keepdims=True)/x_train.shape[1] # calculating the mean for every feature\n",
    "    std = np.sqrt(np.sum(np.power(x_train.to_numpy(),2), axis=1,keepdims=True))/x_train.shape[1] # calculating the standard deviation for every feature\n",
    "    x_train = x_train - mean / (std +epsilon)\n",
    "    return x_train,mean,std,epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the datasets for train and test\n",
    "\n",
    "csv_path = \"C:/Users/anubha/Downloads/digit-recognizer/train.csv\"\n",
    "df=pd.read_csv(csv_path)\n",
    "\n",
    "#Create Y with 0s and 1s corresponding to a particular label\n",
    "#So Y will be of size K * m\n",
    "K=df[\"label\"].value_counts().count()#number of distinct labels\n",
    "m=df.shape[0] # no.of training points\n",
    "    \n",
    "Y = np.zeros((m,K))\n",
    "for i in range(0,m):\n",
    "    j = df.loc[i,\"label\"]\n",
    "    Y[i,j] = 1\n",
    "    \n",
    "#Input\n",
    "X=df.drop('label', 1)\n",
    "\n",
    "#Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=1)\n",
    "x_train = np.transpose(x_train)\n",
    "x_test = np.transpose(x_test)\n",
    "y_train = np.transpose(y_train)\n",
    "y_test = np.transpose(y_test)\n",
    "\n",
    "    \n",
    "# Normalize the input data\n",
    "#x_train,mean,std,epsilon = normalizeInputs(x_train)\n",
    "#x_test = (x_test - mean) / (std + epsilon)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the number of units and the activation function of each unit\n",
    "layers = [X.shape[1],50,50,50,Y.shape[1]] # in X, Y data is not transposed, so columns have features.\n",
    "activations = [\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\"] \n",
    "keep_prob = [1,1,1,1,1]\n",
    "\n",
    "parameters={}\n",
    "for lambd in [0.01]:\n",
    "    print(\"lambd = \",lambd)\n",
    "    #Get the parameters for the model\n",
    "    parameters = model(x_train,y_train,x_test,y_test,activations,layers,lambd,keep_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction and submission\n",
    "test_csv_path=\"C:/Users/anubha/Downloads/digit-recognizer/test.csv\"\n",
    "dftest=pd.read_csv(test_csv_path)\n",
    "X_predict=np.transpose(dftest)\n",
    "#X_predict = (X_predict - mean) / (std + epsilon)\n",
    "keep_probTest = [1] * len(keep_prob)\n",
    "_ ,Yhat =forwardPropagation(X_predict,activations,parameters,keep_probTest)\n",
    "dftest[\"Label\"]=np.argmax(Yhat, axis=0)\n",
    "\n",
    "df_submit=dftest[\"Label\"]\n",
    "df_submit.to_csv(\"C:/Users/anubha/Desktop/digitRecognition.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
